import os
import cv2
import pandas as pd

import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

from .bone import selected_keypoints


point_of_interest = [
    f'{selected_keypoints[key]}_x' for key in  selected_keypoints
] + [
    f'{selected_keypoints[key]}_y' for key  in selected_keypoints
]

#print(point_of_interest)  # ['a_x', 'b_x', 'a_y', 'b_y']


# ëª¨ë¸ ì´ˆê¸°í™”
input_size = 12
num_classes = 3  # í–‰ë™ í´ë˜ìŠ¤ ìˆ˜ :ì—†ìŒ, theft_start, theft_end
#í•œë²ˆì— í•™ìŠµí•  ìƒ˜í”Œ ê°œìˆ˜
batch_length = 18
num_epochs = 30
learning_rate = 0.0005 #í•™ìŠµë¥ 

class SkeletonDataset(Dataset):
    def __init__(self, data, labels, max_len):
        self.data = data
        self.labels = labels
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ì…ë ¥ ë°ì´í„°ì™€ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
        x = self.data[idx]
        y = self.labels[idx]

        # íŒ¨ë”© ì²˜ë¦¬
        if len(x) < self.max_len:
            pad = np.zeros((self.max_len - len(x), x.shape[1]))  # ë¶€ì¡±í•œ ë¶€ë¶„ì„ 0ìœ¼ë¡œ íŒ¨ë”©
            x = np.vstack([x, pad])
        else:
            x = x[:self.max_len]

        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)


class TransformerModel(nn.Module):
    def __init__(self, input_dim, num_classes, num_heads=4, num_layers=5, model_dim=64, dropout=0.1):
        super(TransformerModel, self).__init__()

        # 1ï¸âƒ£ ì…ë ¥ ì°¨ì›(input_dim) â†’ Transformer ëª¨ë¸ ì°¨ì›(model_dim)ìœ¼ë¡œ ë³€í™˜
        self.embedding = nn.Linear(input_dim, model_dim)  # (seq_length, input_dim) -> (seq_length, model_dim)

        # 2ï¸âƒ£ Transformer Encoder ì„¤ì •
        encoder_layers = TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # 3ï¸âƒ£ ë¶„ë¥˜ë¥¼ ìœ„í•œ Fully Connected Layer
        self.fc = nn.Linear(model_dim, num_classes)  # (batch, model_dim) -> (batch, num_classes)

    def forward(self, x):
        """
        x: (seq_length, input_dim)
        """
        #print("Before embedding shape:", x.shape)  # (batch, seq_length, input_dim)
        
        x = self.embedding(x)  # (batch, seq_length, model_dim)
        #print("After embedding shape:", x.shape)

        # âœ… TransformerëŠ” (seq_length, batch, model_dim) í˜•ì‹ í•„ìš” â†’ permute ì‚¬ìš©
        x = x.permute(1, 0, 2)  # (batch, seq_length, model_dim) -> (seq_length, batch, model_dim)
        #print("After permute shape:", x.shape)  # (seq_length, batch, model_dim)

        x = self.transformer_encoder(x)  # (seq_length, batch, model_dim)
        #print("After Transformer shape:", x.shape)

        x = x.mean(dim=0)  # í‰ê·  í’€ë§ (batch, model_dim)
        #print("After pooling shape:", x.shape)  # (batch, model_dim)

        x = self.fc(x)  # (batch, num_classes)
        #print("Final output shape:", x.shape)  # (batch, num_classes)
        return x
        return x


    
class Behavior:
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    
    def predict(predict_images):
        print("--- í–‰ë™ ì˜ˆì¸¡ ì‹œì‘ ---")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = Behavior.load(device)
        #print(predict_images)
        grouped = predict_images.groupby(['video_idx', 'detection_idx']) 


        ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        

        x = []
       
        for group_key, group_df in grouped:
            idx = 0
            while idx < len(group_df):  # âœ… group_df ì‚¬ìš©
                features = np.zeros((input_size, len(point_of_interest)))  # (180, feature ê°œìˆ˜)

                # âœ… `group_df["frame_idx"]`ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
                filtered_df = group_df[(group_df["frame_idx"] >= idx) & 
                                    (group_df["frame_idx"] < idx + input_size)]

                for _, row in filtered_df.iterrows():
                    index = int(row['frame_idx'] - idx)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                    features[index] = row[point_of_interest]  # âœ… `.values` ì œê±°

                x.append(features)
                print(features)
                idx += input_size


            # ë ˆì´ë¸” ì¶”ì¶œ (ìƒ˜í”Œë‹¹ í•˜ë‚˜ì˜ ë ˆì´ë¸”ë¡œ ê³ ì •)
        
        x = np.array(x)  # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        print(f"ì…ë ¥ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {x.shape}")

        # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
        x_tensor = torch.tensor(x, dtype=torch.float32).to(device)
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ì—°ì‚° ë°©ì§€
            predictions = model(x_tensor)
        print(np.array(x).shape)

        # Softmax ì ìš©í•˜ì—¬ í™•ë¥  ë³€í™˜
        probabilities = torch.nn.functional.softmax(predictions, dim=1)

        result = probabilities.cpu().numpy()
     

        #print(result)
        print("--- âœ… í–‰ë™ ì˜ˆì¸¡ ì™„ë£Œ ---")
        return result
      
    def learn(learn_images, learn_labels,model):
        print("--- í–‰ë™ í•™ìŠµ ì‹œì‘ ---")
     
        print(learn_labels)
 
        #print(point_of_interest)
        ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        _filter_theft_frames = Behavior.filter_theft_frames(learn_images, learn_labels)
        #print(_filter_theft_frames.columns)
      
        ## ë¶„ë¦¬í•œ ì ˆë„ ì˜ìƒ í”„ë ˆì„ë“¤ì„ ë‹¤ë“¤ ê°™ì€ ê¸¸ì´ë¡œ ë§ì³ì£¼ëŠ” ê¸°ëŠ¥
        #ì˜ˆ : ì…ë ¥ : [3, 4, 5] , [1, 2, 3, 4, 5, 6, 7]
        #ì˜ˆ : ì¶œë ¥ : [3, 4, 5, 0, 0, 0, 0] , [1, 2, 3, 4, 5, 6, 7]
        # PackedSequenceë¡œ ë³€í™˜
        #packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)

        #print(_filter_theft_frames)

        max_len = 180  # ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
        x = []
        y = []
        sliding_range = []
        used_frames = []

        #ì ˆë„ í–‰ë™ì— ëŒ€í•œ Frameì„ ì¶”ì¶œ
        for now in _filter_theft_frames:
            start = now['start']
            end = now['end']

            sliding_start,sliding_end = Behavior.center_range_with_wrap(start,end,input_size,wrap_limit=max_len)
            sliding_range.append((sliding_start, sliding_end))
            
            used_frames.append((sliding_start, sliding_end))
            
            look_frame = np.zeros((input_size, len(point_of_interest)))  # (18, 18) í¬ê¸°ì˜ 2D ë°°ì—´ ìƒì„±
            filtered_df = learn_images[(learn_images["frame_idx"] >= sliding_start) & 
                           (learn_images["frame_idx"] <= sliding_end)]
            
            for _, row in filtered_df.iterrows():
                idx = int(row['frame_idx'] - sliding_start-1)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                look_frame[idx] = row[point_of_interest]  # âœ… `.values` ì œê±° (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ í•„ìš” ì—†ìŒ)

            x.append(look_frame)
            y.append(now['label'])

        all_frames = set(range(0, max_len))  # ì „ì²´ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ìƒì„±

# ê¸°ì¡´ ë²”ìœ„ ë‚´ í”„ë ˆì„ì„ ìˆ˜ì§‘ (ì œì™¸í•´ì•¼ í•¨)
        

        last_selected = -np.inf  # ì´ˆê¸°ê°’ì„ -ë¬´í•œëŒ€ë¡œ ì„¤ì •
        # 18 í”„ë ˆì„ ê°„ê²©ì„ ìœ ì§€í•˜ë©´ì„œ ì„ íƒ
        selected_frames = []
        i = 0

        #ì¼ë°˜ ì¼€ì´ìŠ¤ì— ëŒ€í•œ frameì„ ì¶”ì¶œ (í•™ìŠµ ì‹œí‚¤ëŠ” ë°ì´í„°ê°€ ì´ìª½ì´ ë§ì„ê±°ë¼ ì ˆë„ í–‰ìœ„ ì¼€ì´ìŠ¤ë‘ ìˆ˜ë¥¼ ì ì ˆíˆ ì¡°ì ˆì´ í•„ìš”í• ìˆ˜ë„.)
        while(i < max_len):
            start = i 
            end = i + input_size - 1
            able = True
            # ìŠ¬ë¼ì´ë”© ë²”ìœ„ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            for sliding_start, sliding_end in sliding_range:
                if not (end < sliding_start or start > sliding_end):  # ê²¹ì¹˜ëŠ” ê²½ìš°
                    able = False
                    break  # í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì„ íƒ ë¶ˆê°€

            if able:
                look_frame = np.zeros((input_size, len(point_of_interest)))  # (18, 18) í¬ê¸°ì˜ 2D ë°°ì—´ ìƒì„±
                filtered_df = learn_images[(learn_images["frame_idx"] >= start) & 
                            (learn_images["frame_idx"] <= end + 1)]
                
                for _, row in filtered_df.iterrows():
                    idx = int(row['frame_idx'] - start-1)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                    look_frame[idx] = row[point_of_interest]  # âœ… `.values` ì œê±° (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ í•„ìš” ì—†ìŒ)

                x.append(look_frame)
                y.append(0)  # ë ˆì´ë¸” ì—†ìŒ

            i += input_size  # ë‹¤ìŒ ìœˆë„ìš° ì´ë™

        #print(y)
   


        
        # ë ˆì´ë¸” ì¶”ì¶œ (ìƒ˜í”Œë‹¹ í•˜ë‚˜ì˜ ë ˆì´ë¸”ë¡œ ê³ ì •)
        
    
        #print(np.array(x).shape)
        #print(np.array(y).shape)
        #MKLDNNì€ ê³ ì† CPU ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ë° GPU í™˜ê²½ì—ì„  í•„ìš”ì—†ìŒ.
        torch.backends.mkldnn.enabled = False
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f'device : {device}')
        dataset = SkeletonDataset(x, y, input_size)

        # ì‚¬ìš©í•  ë°ì´í„°ì…‹ (PyTorch Dataset ê°ì²´)
        dataloader = DataLoader(dataset, batch_size=batch_length, shuffle=True)
        # pin_memory,    # GPUë¡œ ë¹ ë¥´ê²Œ ë¡œë“œí• ì§€ ì—¬ë¶€

        
        #model = LSTMModel(input_size, hidden_size, num_layers, num_classes)
        if model is None:
            model = TransformerModel(input_dim=len(point_of_interest),num_heads=16, num_classes=3)   
            model.to(device)   

        # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.0001)
       
        
            
       
        first_fc_weights = model.fc.weight.clone().detach().cpu().numpy()  # ì´ˆê¸° FC ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ì €ì¥

     
        print("-- í›ˆë ¨ ì‹œì‘ ---")
        for epoch in range(num_epochs):
            print(f"-- Epochs : {epoch} ---")
            model.train()
            

            #ëª¨ë“  ë°ì´í„°ì…‹ì„ ìˆœí™˜í•˜ì§€ë§Œ GPUì— ë°°ì¹˜ í¬ê¸° ë³„ë¡œ ë„£ì–´ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹
            for batch_idx ,( inputs, targets) in enumerate(dataloader):
                
                # ì…ë ¥ê³¼ ì •ë‹µ ë ˆì´ë¸”ì„ ì¥ì¹˜ë¡œ ì´ë™
                inputs, targets = inputs.to(device), targets.to(device)
            
                print(f"ì…ë ¥ ë°ì´í„° : {inputs.shape}")  # ì˜ˆìƒ: íŠ¹ì§• ìˆ˜  batch_length ,input_size , = [2, 18, 180]
                print(f"íƒ€ê²Ÿ ë°ì´í„° : {targets.shape}")  # ì˜ˆìƒ:  = [2, 180]

                    
                    # ìˆœì „íŒŒ
                    
                output = model(inputs)
                    

                    #criterion ì˜ í˜•ì‹ì´  outputs  : (N, num_classes)
                    #targets : (N,) ì´ë¯€ë¡œ  
                    #ì§€ì› ì°¨ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ Nì€ batch_size Ã— seq_lenìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨.
              
                print(output.shape)
                print(f"Reshaped outputs shape: {output.shape}")  # ì˜ˆìƒ: (144, 3)
                    #print(f"Reshaped targets shape: {targets.shape}")  # ì˜ˆìƒ: (144,)

                loss = criterion(output, targets)

                    # ì—­ì „íŒŒ ë° ìµœì í™”
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                my_loss = loss.item()

                current_fc_weights = model.fc.weight.clone().detach().cpu().numpy()
                weight_change = np.abs(current_fc_weights - first_fc_weights).sum()
                print(f"ğŸ”„ FC ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰ (Batch {batch_idx}): {weight_change} / loss : {my_loss}")

                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëœ ê²½ìš° ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ ê°±ì‹ 
                first_fc_weights = current_fc_weights
                
        print("--- í–‰ë™ í•™ìŠµ ì™„ë£Œ ---")   
        return model;
    
    def load(device):
        """ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
        model_path = "./export/model/theft_detection_model.pth"
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        

        # ëª¨ë¸ ì´ˆê¸°í™” (ì…ë ¥ í¬ê¸° ë° í´ë˜ìŠ¤ ê°œìˆ˜ ì„¤ì •)
        
        model = TransformerModel(input_dim=len(point_of_interest),num_heads=16, num_classes=3)      
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)
        model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

        print(f"ëª¨ë¸ì´ {model_path} ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return model
    def save(model_dict):
        save_dir = "./export/model"
        os.makedirs(save_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        torch.save(model_dict, f"{save_dir}/theft_detection_model.pth")
    ## ì ˆë„ í–‰ë™ì„ 60í”„ë ˆì„ì˜ ì¤‘ì•™ì— ë°°ì¹˜í•˜ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ ìƒì„±
    def center_range_with_wrap(start, end, array_size=60, wrap_limit=180):
        #18
        shift = (array_size - (end-start)) // 2
        new_start = start - shift
        new_end = end + shift
        # ì´ˆê³¼ë¶„ ë°˜ëŒ€ìª½ìœ¼ë¡œ ì—°ì¥ ì²˜ë¦¬
        if new_start < 0:
            overflow = abs(new_start)
            new_start = 0
            new_end += overflow  # ì´ˆê³¼ëœ ë¶€ë¶„ì„ ëìª½ìœ¼ë¡œ ì¶”ê°€

        if new_end >= wrap_limit:
            overflow = new_end - (wrap_limit - 1)
            new_end = wrap_limit - 1
            new_start -= overflow  # ì´ˆê³¼ëœ ë¶€ë¶„ì„ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ë³´ì •

        return new_start,new_end
    ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    def filter_theft_frames(learn_images, learn_labels):
        conversion_frames = []

        
        labelings = ['theft_start','theft_end']

        filtered_df = learn_labels[learn_labels['type'] == 'box']
        print(filtered_df)
        for group in filtered_df:
            
            for label_idx , labeling in enumerate( labelings):
                labeling_frames = filtered_df[
                    (filtered_df['label'] == labeling) &
                    (filtered_df['video_idx'].notnull()) 
                ]
                if not labeling_frames.empty:
                    min_frame_row = labeling_frames.loc[labeling_frames['frame_idx'].idxmin()]
                    max_frame_row = labeling_frames.loc[labeling_frames['frame_idx'].idxmax()]

                    conversion_frames.append({'video_idx':min_frame_row['video_idx'],'start':min_frame_row['frame_idx'],'end':max_frame_row['frame_idx'],'label':label_idx+1})
 
        return conversion_frames
        