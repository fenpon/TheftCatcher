import os
import cv2
import pandas as pd

import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler
import numpy as np

from .bone import selected_keypoints


point_of_interest = [
    f'{selected_keypoints[key]}_x' for key in  selected_keypoints
] + [
    f'{selected_keypoints[key]}_y' for key  in selected_keypoints
]

#print(point_of_interest)  # ['a_x', 'b_x', 'a_y', 'b_y']


# ëª¨ë¸ ì´ˆê¸°í™”
input_size = 10
num_classes = 2  # í–‰ë™ í´ë˜ìŠ¤ ìˆ˜ :ì—†ìŒ, theft_start, theft_end
#í•œë²ˆì— í•™ìŠµí•  ìƒ˜í”Œ ê°œìˆ˜
batch_length = 18
num_epochs = 30
learning_rate = 0.0005 #í•™ìŠµë¥ 


def get_weighted_sampler(labels):
    class_counts = np.bincount(labels)
    class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float32)
    sample_weights = [class_weights[label] for label in labels]
    return WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            torch.nn.init.zeros_(m.bias)
    
    elif isinstance(m, nn.Embedding):  # Embedding ì´ˆê¸°í™”
        torch.nn.init.xavier_uniform_(m.weight)

    elif isinstance(m, nn.LayerNorm):  # LayerNorm ì´ˆê¸°í™”
        torch.nn.init.ones_(m.weight)
        torch.nn.init.zeros_(m.bias)

    elif isinstance(m, nn.MultiheadAttention):  # MultiheadAttention ì´ˆê¸°í™”
        torch.nn.init.xavier_uniform_(m.in_proj_weight)
        torch.nn.init.zeros_(m.in_proj_bias)


class SkeletonDataset(Dataset):
    def __init__(self, data, labels, max_len):
        self.data = data
        self.labels = labels
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # ì…ë ¥ ë°ì´í„°ì™€ ë ˆì´ë¸” ê°€ì ¸ì˜¤ê¸°
        x = self.data[idx]
        y = self.labels[idx]

        # íŒ¨ë”© ì²˜ë¦¬
        if len(x) < self.max_len:
            pad = np.zeros((self.max_len - len(x), x.shape[1]))  # ë¶€ì¡±í•œ ë¶€ë¶„ì„ 0ìœ¼ë¡œ íŒ¨ë”©
            x = np.vstack([x, pad])
        else:
            x = x[:self.max_len]

        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)


class TransformerModel(nn.Module):
    def __init__(self, input_dim=12, num_classes=2, num_heads=6, num_layers=4, model_dim=24, dropout=0.1):
        super(TransformerModel, self).__init__()
         # MLP ê¸°ë°˜ Feature Encoding
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_dim * 3, model_dim),  # ğŸ”¹ input_dim â†’ input_dim * 3 (12 â†’ 36)
            nn.ReLU(),
            nn.BatchNorm1d(model_dim),
            nn.Linear(model_dim, model_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        # ì…ë ¥ ì°¨ì›(input_dim) â†’ Transformer ëª¨ë¸ ì°¨ì›(model_dim)ìœ¼ë¡œ ë³€í™˜
        self.embedding = nn.Linear(model_dim, model_dim)  # (seq_length, input_dim) -> (seq_length, model_dim)

        # Transformer Encoder ì„¤ì •
        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers, norm=nn.LayerNorm(model_dim))

        # Fully Connected Layer (Classification)
        self.fc = nn.Sequential(
            nn.Linear(model_dim * 2, model_dim),
            nn.ReLU(),
            nn.Linear(model_dim, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        """
        [[0.4879, 0.5364, 0.5850, 0.7307, 0.6093, 0.6336, 0.6318, 0.6318, 0.4854, 0.4463, 0.3096, 0.2803],
        [0.4865, 0.4326, 0.6213, 0.3517, 0.6483, 0.4865, 0.6318, 0.6221, 0.5049, 0.4561, 0.3096, 0.2998],
        [0.5301, 0.4498, 0.7110, 0.5301, 0.7110, 0.5502, 0.6123, 0.6025, 0.4854, 0.4463, 0.2998, 0.2998],
        [0.3511, 0.3114, 0.4702, 0.3114, 0.4901, 0.3511, 0.6123, 0.6123, 0.4756, 0.4463, 0.2998, 0.2998]]
        -> í•œ ì‹œí€€ìŠ¤ë¥¼ ëŒ€í‘œí•˜ëŠ” íŠ¹ì§•ë§Œ ë½‘ì•„ì„œ 
        [0.3, 0.7]
        label ë¶„ë¥˜ í™•ë¥ ì„ ë§Œë“¤ì–´ì¤€ë‹¤.
        """

        # ğŸ”¹ ì´ë™ëŸ‰ ì°¨ì´(Î”X)
        delta_x = torch.diff(x, dim=1, prepend=torch.zeros_like(x[:, :1]))

        # ğŸ”¹ ê°€ì†ë„ ì°¨ì´(Î”Î”X)
        delta2_x = torch.diff(delta_x, dim=1, prepend=torch.zeros_like(delta_x[:, :1]))

        # ğŸ”¹ ì›ë³¸ ë°ì´í„°(X)ì™€ ê²°í•©í•˜ì—¬ íŠ¹ì§• ê°•í™”
        x = torch.cat([x, delta_x, delta2_x], dim=-1)  # (batch, seq_length, feature_dim * 3)

        # ğŸ”¹ Feature Encoding (MLP)
        batch_size, seq_len, feature_dim = x.shape
        x = x.view(batch_size * seq_len, feature_dim)  # ğŸ”¹ BatchNorm1d ì ìš©ì„ ìœ„í•´ reshape
        x = self.feature_encoder(x)
        x = x.view(batch_size, seq_len, -1)  # ğŸ”¹ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›

        # ğŸ”¹ Transformer Encoder ì ìš©
        x = self.transformer_encoder(x)

        # ğŸ”¹ ì‹œí€€ìŠ¤ íŠ¹ì§• ìš”ì•½ (Mean + Max Pooling)
        x_mean = torch.mean(x, dim=1)
        x_max = torch.max(x, dim=1)[0]
        x = torch.cat([x_mean, x_max], dim=1)  # (batch, model_dim * 2)

        # ğŸ”¹ ìµœì¢… ë¶„ë¥˜
        x = self.fc(x)  # (batch, num_classes)

        return x


    
class Behavior:
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    
    def predict(predict_images):
        print("--- í–‰ë™ ì˜ˆì¸¡ ì‹œì‘ ---")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = Behavior.load(device)

        ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        
        dd = []
       
        max_len = 180  # ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
        i = 0
          
        print(predict_images)       
        result = []
        sliding_range = []

        grouped = predict_images.groupby(['video_idx', 'detection_idx'])
        for (video_idx,detection_idx), group in grouped:  # key: ('video_idx ê°’', 'detection_idx ê°’'), group: í•´ë‹¹ ê·¸ë£¹ ë°ì´í„°í”„ë ˆì„
            x = []
            x_i = []
            for i in range(0, max_len, input_size):
                #print("G : ",(i))
                arr = [[0 for jj in range(len(point_of_interest))] for ii in range(input_size)]
                now = group[(group["frame_idx"] < (i + input_size )) & (group["frame_idx"] >= i)]

                is_able = False
                for _, row in now.iterrows():
                    idx = int(row['frame_idx'] - i)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                    arr[idx] = row[point_of_interest].values
                    is_able = True

                if not is_able:
                    continue
                x.append(arr)
                x_i.append([i,detection_idx])
                
            # ë¦¬ìŠ¤íŠ¸ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ (ë°°ì—´ í¬ê¸°ê°€ ì¼ì •í•´ì•¼ í•¨)
            x_array = np.stack(x, axis=0).astype(np.float32)  # âš¡ `np.stack()`ì„ ì‚¬ìš©í•˜ì—¬ ë³€í™˜

            # PyTorch Tensor ë³€í™˜ + GPU ì´ë™
            x_tensor = torch.tensor(x_array, dtype=torch.float32).to(device)  # âš¡ GPU ì´ë™

            with torch.no_grad():  # ëª¨ë¸ ì¶”ë¡  ì‹œ ë¶ˆí•„ìš”í•œ gradient ê³„ì‚° ë°©ì§€
                    output = model(x_tensor)  # ëª¨ë¸ ì˜ˆì¸¡
                    ratio = torch.nn.functional.softmax(output, dim=1)  # í™•ë¥  ê°’ ë³€í™˜
            ratio = ratio.cpu().numpy()  # GPU ì‚¬ìš© ì‹œ CPUë¡œ ì´ë™ í›„ ì¶œë ¥

            for i in range(len(ratio)):
                if ratio[i][1] >= 0.6:
                    print(x_i[i])
                    for j in range(0, input_size):
                        val = x_i[i]
                        result.append((val[0]+j,val[1]))
            print(ratio)  
                #print(now)
                #now = now.reset_index(drop=True)
              

                
                        #if ratio[ratio_index][1] >= 0.8:
                           # result.append(frame_idx.values)
                        #j += input_size
              

        #print(result)
        print("--- âœ… í–‰ë™ ì˜ˆì¸¡ ì™„ë£Œ ---")
        return result
      
    def learn(learn_images, learn_labels,model):
        print("--- í–‰ë™ í•™ìŠµ ì‹œì‘ ---")
     
        #print(learn_labels)
 
        #print(point_of_interest)
        ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        _filter_theft_frames = Behavior.filter_theft_frames(learn_images, learn_labels)
        #print(_filter_theft_frames.columns)
      
        ## ë¶„ë¦¬í•œ ì ˆë„ ì˜ìƒ í”„ë ˆì„ë“¤ì„ ë‹¤ë“¤ ê°™ì€ ê¸¸ì´ë¡œ ë§ì³ì£¼ëŠ” ê¸°ëŠ¥
        #ì˜ˆ : ì…ë ¥ : [3, 4, 5] , [1, 2, 3, 4, 5, 6, 7]
        #ì˜ˆ : ì¶œë ¥ : [3, 4, 5, 0, 0, 0, 0] , [1, 2, 3, 4, 5, 6, 7]
        # PackedSequenceë¡œ ë³€í™˜
        #packed = pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False)

        #print(_filter_theft_frames)

        max_len = 180  # ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´
        x = []
        y = []
        sliding_range = []
        used_frames = []
        print(_filter_theft_frames)

        kkkk = 0
        theft_features =[]
        #ì ˆë„ í–‰ë™ì— ëŒ€í•œ Frameì„ ì¶”ì¶œ
        for now in _filter_theft_frames:
            start = now['start']
            end = now['end']

            sliding_start,sliding_end = Behavior.center_range_with_wrap(start,end,input_size,wrap_limit=max_len)
            sliding_range.append((sliding_start, sliding_end))
            
            used_frames.append((sliding_start, sliding_end))
            
            look_frame = np.zeros((input_size, len(point_of_interest)))  # (18, 18) í¬ê¸°ì˜ 2D ë°°ì—´ ìƒì„±
            filtered_df = learn_images[(learn_images["frame_idx"] >= sliding_start) & 
                           (learn_images["frame_idx"] <= sliding_end)]
            
            #print(filtered_df)
            for _, row in filtered_df.iterrows():
                idx = int(row['frame_idx'] - sliding_start-1)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                look_frame[idx] = row[point_of_interest]  # âœ… `.values` ì œê±° (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ í•„ìš” ì—†ìŒ)
        
            
            kkkk += len(look_frame)
            theft_features.append([now['label'],look_frame])
    

        all_frames = set(range(0, max_len))  # ì „ì²´ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ìƒì„±

# ê¸°ì¡´ ë²”ìœ„ ë‚´ í”„ë ˆì„ì„ ìˆ˜ì§‘ (ì œì™¸í•´ì•¼ í•¨)
        

        last_selected = -np.inf  # ì´ˆê¸°ê°’ì„ -ë¬´í•œëŒ€ë¡œ ì„¤ì •
        # 18 í”„ë ˆì„ ê°„ê²©ì„ ìœ ì§€í•˜ë©´ì„œ ì„ íƒ
        selected_frames = []
        i = 0

        jjjj = 0
        genenal_features = []
        #ì¼ë°˜ ì¼€ì´ìŠ¤ì— ëŒ€í•œ frameì„ ì¶”ì¶œ (í•™ìŠµ ì‹œí‚¤ëŠ” ë°ì´í„°ê°€ ì´ìª½ì´ ë§ì„ê±°ë¼ ì ˆë„ í–‰ìœ„ ì¼€ì´ìŠ¤ë‘ ìˆ˜ë¥¼ ì ì ˆíˆ ì¡°ì ˆì´ í•„ìš”í• ìˆ˜ë„.)
        while(i < max_len):
            start = i 
            end = i + input_size - 1
            able = True
            # ìŠ¬ë¼ì´ë”© ë²”ìœ„ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            for sliding_start, sliding_end in sliding_range:
                if not (end < sliding_start or start > sliding_end):  # ê²¹ì¹˜ëŠ” ê²½ìš°
                    able = False
                    break  # í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì„ íƒ ë¶ˆê°€

            if able:
                look_frame = np.zeros((input_size, len(point_of_interest)))  # (18, 18) í¬ê¸°ì˜ 2D ë°°ì—´ ìƒì„±
                filtered_df = learn_images[(learn_images["frame_idx"] >= start) & 
                            (learn_images["frame_idx"] <= end + 1)]
                
                for _, row in filtered_df.iterrows():
                    idx = int(row['frame_idx'] - start-1)  # âœ… `int()` ë³€í™˜ í•„ìˆ˜
                    look_frame[idx] = row[point_of_interest]  # âœ… `.values` ì œê±° (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ í•„ìš” ì—†ìŒ)
                jjjj += len(look_frame)
                theft_features.append([0,look_frame])

            i += input_size  # ë‹¤ìŒ ìœˆë„ìš° ì´ë™


        # ë‘ ë°°ì—´ì„ í•©ì¹˜ê¸° (ì°¨ì› ìœ ì§€)
        merged_features = genenal_features + theft_features


        torch.backends.mkldnn.enabled = False
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f'device : {device}')
        x = [row[1] for row in merged_features]  # ì…ë ¥ ë°ì´í„°
        y = [row[0] for row in merged_features]  # ì…ë ¥ ë°ì´í„°
    
        y = torch.tensor(y, dtype=torch.long)  # ë¦¬ìŠ¤íŠ¸ë¥¼ Long Tensorë¡œ ë³€í™˜

        # ê° í´ë˜ìŠ¤(0, 1)ì˜ ê°œìˆ˜ í™•ì¸
        class_counts = np.bincount(y)
        class_weights = 1.0 / class_counts # ê°œìˆ˜ê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        sample_weights = class_weights[y]

        # WeightedRandomSampler ìƒì„±
        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(y), replacement=True)

        dataset = SkeletonDataset(x, y, input_size)
       
        # ì‚¬ìš©í•  ë°ì´í„°ì…‹ (PyTorch Dataset ê°ì²´)
        dataloader = DataLoader(dataset, batch_size=batch_length,sampler=sampler)
        # pin_memory,    # GPUë¡œ ë¹ ë¥´ê²Œ ë¡œë“œí• ì§€ ì—¬ë¶€

        
        #model = LSTMModel(input_size, hidden_size, num_layers, num_classes)
        if model is None:
            model = TransformerModel()  
            model.apply(init_weights) 
            model.to(device)   
            
        #ì ˆë„í–‰ìœ„ í•™ìŠµ ê°•ì¡°
        #ëŒ€ë¶€ë¶„ì˜ ì˜ˆì¸¡ì´ 0(ë¹„ì ˆë„)ë¡œ í¸í–¥ë¨. ì´ë¥¼ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬
        weight = torch.tensor([0.3,0.7])
        
        # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
        criterion = nn.CrossEntropyLoss(weight=weight).to(device)#ì •ìƒ í–‰ë™, ì ˆë„ í–‰ìœ„
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0.001, amsgrad=True)
     
        print("-- í›ˆë ¨ ì‹œì‘ ---")
        for epoch in range(num_epochs):
            print(f"-- Epochs : {epoch} ---")
            model.train()
            
            print(f"// --- Epochs {epoch} --- //")
            #ëª¨ë“  ë°ì´í„°ì…‹ì„ ìˆœí™˜í•˜ì§€ë§Œ GPUì— ë°°ì¹˜ í¬ê¸° ë³„ë¡œ ë„£ì–´ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹
            for batch_idx ,( inputs, targets) in enumerate(dataloader):
                        inputs, targets = inputs.to(device), targets.to(device)
                    
                        # ì…ë ¥ê³¼ ì •ë‹µ ë ˆì´ë¸”ì„ ì¥ì¹˜ë¡œ ì´ë™
                        #print(inputs)
                        
                        # ìˆœì „íŒŒ
                        optimizer.zero_grad()
                        output = model(inputs)
                        ratio = torch.nn.functional.softmax(output, dim=1)  # í™•ë¥  ê°’ ë°˜í™˜   
                        print(targets)
                        print(ratio) 
                        labels = [targets[i] for i in range(len(output))]
                        # Convert labels to a tensor if they are a list
                        if isinstance(labels, list):
                            labels = torch.tensor(labels, dtype=torch.long)
                        labels = labels.to(device)

                        #print(labels)
                        loss = criterion(output, labels)
                        # ì—­ì „íŒŒ ë° ìµœì í™”
                                             
                        loss.backward()
                        optimizer.step()
                        print(f"Loss : {loss.item()}")
        print(f"í•™ìŠµ ê°€ì¤‘ì¹˜ : {weight}")
        print("--- í–‰ë™ í•™ìŠµ ì™„ë£Œ ---")   
        return model;
    
    def load(device):
        """ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
        model_path = "./export/model/theft_detection_model.pth"
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        

        # ëª¨ë¸ ì´ˆê¸°í™” (ì…ë ¥ í¬ê¸° ë° í´ë˜ìŠ¤ ê°œìˆ˜ ì„¤ì •)
        
        model = TransformerModel()      
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.to(device)
        model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

        print(f"ëª¨ë¸ì´ {model_path} ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return model
    def save(model_dict):
        save_dir = "./export/model"
        print("ëª¨ë¸ ì €ì¥")
        os.makedirs(save_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        torch.save(model_dict, f"{save_dir}/theft_detection_model.pth")
    ## ì ˆë„ í–‰ë™ì„ 60í”„ë ˆì„ì˜ ì¤‘ì•™ì— ë°°ì¹˜í•˜ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ ìƒì„±
    def center_range_with_wrap(start, end, array_size=60, wrap_limit=180):
        #18
        shift = (array_size - (end-start)) // 2
        new_start = start - shift
        new_end = end + shift
        # ì´ˆê³¼ë¶„ ë°˜ëŒ€ìª½ìœ¼ë¡œ ì—°ì¥ ì²˜ë¦¬
        if new_start < 0:
            overflow = abs(new_start)
            new_start = 0
            new_end += overflow  # ì´ˆê³¼ëœ ë¶€ë¶„ì„ ëìª½ìœ¼ë¡œ ì¶”ê°€

        if new_end >= wrap_limit:
            overflow = new_end - (wrap_limit - 1)
            new_end = wrap_limit - 1
            new_start -= overflow  # ì´ˆê³¼ëœ ë¶€ë¶„ì„ ì‹œì‘ ë¶€ë¶„ìœ¼ë¡œ ë³´ì •

        return new_start,new_end
    ## ê° ê°œì²´ë³„ë¡œ ì ˆë„ ë¼ë²¨ë§ ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì„œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    import pandas as pd

    def filter_theft_frames(learn_images, learn_labels):
        conversion_frames = []
        
        # 'theft_end' ë ˆì´ë¸”ì„ ì°¾ìŒ
        labelings = ['theft_end']
        #print(learn_labels)
        # ì—¬ëŸ¬ ê°œì˜ DataFrameì„ í•˜ë‚˜ë¡œ ë³‘í•©
        convert_df = pd.concat(learn_labels, ignore_index=False)

        # 'box' íƒ€ì…ë§Œ í•„í„°ë§
        convert_df = convert_df[convert_df['type'] == 'box']

        for idx, labeling in enumerate( labelings):
            # íŠ¹ì • ë¼ë²¨ë§Œ í•„í„°ë§
            labeling_frames = convert_df[
                (convert_df['label'] == labeling) & (convert_df['video_idx'].notnull())
            ]
            start, end = labeling_frames['frame_idx'].min(), labeling_frames['frame_idx'].max()

            conversion_frames.append({"start":start,"end": end, 'label': idx+1})
            #print(labeling_frames)

        return conversion_frames

        